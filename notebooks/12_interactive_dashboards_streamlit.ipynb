{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6e7e5aa",
   "metadata": {},
   "source": [
    "# Module 12: Interactive Dashboards and Streamlit\n",
    "\n",
    "## ğŸš€ Building Dynamic Data Applications\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Master Streamlit for rapid dashboard development\n",
    "- Create interactive visualizations with user controls\n",
    "- Build multi-page data applications with navigation\n",
    "- Implement real-time data updates and filtering\n",
    "- Deploy dashboards for sharing and collaboration\n",
    "- Optimize performance for large datasets and multiple users\n",
    "\n",
    "**Topics Covered:**\n",
    "1. **Streamlit Fundamentals** - Layout, widgets, and basic interactivity\n",
    "2. **Advanced Components** - Custom widgets, multi-select, date ranges\n",
    "3. **Data Integration** - File uploads, database connections, APIs\n",
    "4. **Interactive Plotting** - Plotly integration, dynamic updates\n",
    "5. **Multi-Page Apps** - Navigation, state management, user sessions\n",
    "6. **Performance Optimization** - Caching, lazy loading, efficient updates\n",
    "\n",
    "---\n",
    "\n",
    "### Why Interactive Dashboards Matter\n",
    "\n",
    "- **User Engagement**: Interactive elements encourage data exploration\n",
    "- **Self-Service Analytics**: Enable stakeholders to answer their own questions\n",
    "- **Real-Time Insights**: Dynamic updates provide current information\n",
    "- **Accessibility**: Web-based dashboards work across devices and platforms\n",
    "\n",
    "Let's build professional interactive dashboards! ğŸ¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0143cbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Interactive Dashboard Libraries Loaded!\n",
      "âœ… Streamlit version: 1.48.1\n",
      "âœ… Plotly version: 6.3.0\n",
      "âœ… Ready to build interactive dashboards!\n",
      "ğŸ“Š Sample Datasets Created:\n",
      "   â€¢ Sales Data: 731 records from 2023-01-01 00:00:00 to 2024-12-31 00:00:00\n",
      "   â€¢ Iris Dataset: 150 botanical measurements\n",
      "   â€¢ Customer Analytics: 1,000 customer profiles\n",
      "âœ… Ready for interactive dashboard development!\n"
     ]
    }
   ],
   "source": [
    "# Essential libraries for interactive dashboards\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Dashboard styling and utilities\n",
    "import io\n",
    "import base64\n",
    "from PIL import Image\n",
    "\n",
    "# Data sources\n",
    "from sklearn.datasets import load_iris, load_wine, make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(\"ğŸš€ Interactive Dashboard Libraries Loaded!\")\n",
    "print(\"âœ… Streamlit version:\", st.__version__)\n",
    "import plotly\n",
    "print(\"âœ… Plotly version:\", plotly.__version__)\n",
    "print(\"âœ… Ready to build interactive dashboards!\")\n",
    "\n",
    "# Create comprehensive sample datasets for dashboard demonstrations\n",
    "def load_dashboard_data():\n",
    "    \"\"\"Load and prepare sample datasets for dashboard examples\"\"\"\n",
    "    \n",
    "    # Dataset 1: Sales data with time series\n",
    "    np.random.seed(42)\n",
    "    dates = pd.date_range('2023-01-01', '2024-12-31', freq='D')\n",
    "    n_days = len(dates)\n",
    "    \n",
    "    # Simulate realistic sales patterns\n",
    "    trend = np.linspace(1000, 1500, n_days)\n",
    "    seasonality = 200 * np.sin(2 * np.pi * np.arange(n_days) / 365.25)\n",
    "    weekly_pattern = 100 * np.sin(2 * np.pi * np.arange(n_days) / 7)\n",
    "    noise = np.random.normal(0, 50, n_days)\n",
    "    \n",
    "    sales_data = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'sales': trend + seasonality + weekly_pattern + noise,\n",
    "        'region': np.random.choice(['North', 'South', 'East', 'West'], n_days),\n",
    "        'product': np.random.choice(['A', 'B', 'C', 'D'], n_days),\n",
    "        'sales_rep': np.random.choice([f'Rep_{i}' for i in range(1, 11)], n_days)\n",
    "    })\n",
    "    sales_data['sales'] = np.maximum(sales_data['sales'], 100)  # Ensure positive sales\n",
    "    \n",
    "    # Dataset 2: Enhanced Iris dataset\n",
    "    iris = load_iris()\n",
    "    iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "    iris_df['species'] = iris.target_names[iris.target]\n",
    "    iris_df['measurement_date'] = pd.date_range('2024-01-01', periods=len(iris_df), freq='D')\n",
    "    iris_df['researcher'] = np.random.choice(['Dr. Smith', 'Dr. Johnson', 'Dr. Brown'], len(iris_df))\n",
    "    \n",
    "    # Dataset 3: Customer analytics data\n",
    "    n_customers = 1000\n",
    "    customer_data = pd.DataFrame({\n",
    "        'customer_id': range(1, n_customers + 1),\n",
    "        'age': np.random.normal(40, 15, n_customers).astype(int),\n",
    "        'income': np.random.lognormal(10.5, 0.5, n_customers),\n",
    "        'spending': np.random.gamma(2, 500, n_customers),\n",
    "        'satisfaction': np.random.beta(3, 1, n_customers) * 5,\n",
    "        'segment': np.random.choice(['Premium', 'Standard', 'Basic'], n_customers, p=[0.2, 0.5, 0.3]),\n",
    "        'signup_date': pd.date_range('2020-01-01', '2024-01-01', periods=n_customers)\n",
    "    })\n",
    "    customer_data['age'] = np.clip(customer_data['age'], 18, 80)\n",
    "    \n",
    "    return sales_data, iris_df, customer_data\n",
    "\n",
    "# Load the datasets\n",
    "sales_data, iris_df, customer_data = load_dashboard_data()\n",
    "\n",
    "print(\"ğŸ“Š Sample Datasets Created:\")\n",
    "print(f\"   â€¢ Sales Data: {len(sales_data):,} records from {sales_data['date'].min()} to {sales_data['date'].max()}\")\n",
    "print(f\"   â€¢ Iris Dataset: {len(iris_df)} botanical measurements\")\n",
    "print(f\"   â€¢ Customer Analytics: {len(customer_data):,} customer profiles\")\n",
    "print(\"âœ… Ready for interactive dashboard development!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd774f9",
   "metadata": {},
   "source": [
    "## ğŸ® Creating Interactive Streamlit Dashboards\n",
    "\n",
    "Since Streamlit apps run as web applications, we'll create dashboard code files that can be launched separately. Let's build several examples showcasing different dashboard patterns and capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad54f56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Sales Analytics Dashboard Created!\n",
      "   ğŸ“ File: /Users/sanjeevadodlapati/Downloads/Repos/DataVisualization-Comprehensive-Tutorial/outputs/dashboards/sales_dashboard.py\n",
      "   ğŸš€ To run: streamlit run sales_dashboard.py\n",
      "\n",
      "ğŸ”¬ Scientific Data Explorer Created!\n",
      "   ğŸ“ File: /Users/sanjeevadodlapati/Downloads/Repos/DataVisualization-Comprehensive-Tutorial/outputs/dashboards/scientific_explorer.py\n",
      "   ğŸš€ To run: streamlit run scientific_explorer.py\n",
      "\n",
      "ğŸ“š Multi-Page Dashboard Template Created!\n",
      "   ğŸ“ File: /Users/sanjeevadodlapati/Downloads/Repos/DataVisualization-Comprehensive-Tutorial/outputs/dashboards/multipage_dashboard.py\n",
      "   ğŸš€ To run: streamlit run multipage_dashboard.py\n",
      "\n",
      "âœ… All Dashboard Examples Created Successfully!\n",
      "ğŸ“ Dashboard files location: /Users/sanjeevadodlapati/Downloads/Repos/DataVisualization-Comprehensive-Tutorial/outputs/dashboards\n",
      "\n",
      "ğŸš€ To run any dashboard:\n",
      "   1. Open terminal in the dashboard directory\n",
      "   2. Run: streamlit run <dashboard_name>.py\n",
      "   3. Dashboard will open in your web browser\n",
      "\n",
      "ğŸ“‹ Available Dashboards:\n",
      "   â€¢ sales_dashboard.py - Sales analytics with filters and KPIs\n",
      "   â€¢ scientific_explorer.py - Scientific data analysis with ML\n",
      "   â€¢ multipage_dashboard.py - Multi-page app template\n"
     ]
    }
   ],
   "source": [
    "# Create dashboard directory\n",
    "import os\n",
    "dashboard_dir = '/Users/sanjeevadodlapati/Downloads/Repos/DataVisualization-Comprehensive-Tutorial/outputs/dashboards'\n",
    "os.makedirs(dashboard_dir, exist_ok=True)\n",
    "\n",
    "# Dashboard 1: Sales Analytics Dashboard\n",
    "sales_dashboard_code = '''\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Configure page\n",
    "st.set_page_config(\n",
    "    page_title=\"Sales Analytics Dashboard\",\n",
    "    page_icon=\"ğŸ“Š\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\"\n",
    ")\n",
    "\n",
    "# Custom CSS for styling\n",
    "st.markdown(\"\"\"\n",
    "<style>\n",
    "    .main-header {\n",
    "        font-size: 3rem;\n",
    "        color: #1f77b4;\n",
    "        text-align: center;\n",
    "        margin-bottom: 2rem;\n",
    "    }\n",
    "    .metric-card {\n",
    "        background-color: #f0f2f6;\n",
    "        padding: 1rem;\n",
    "        border-radius: 0.5rem;\n",
    "        margin: 0.5rem 0;\n",
    "    }\n",
    "    .sidebar .sidebar-content {\n",
    "        background-color: #262730;\n",
    "    }\n",
    "</style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# Sample data generation (same as in notebook)\n",
    "@st.cache_data\n",
    "def load_sales_data():\n",
    "    np.random.seed(42)\n",
    "    dates = pd.date_range('2023-01-01', '2024-12-31', freq='D')\n",
    "    n_days = len(dates)\n",
    "    \n",
    "    trend = np.linspace(1000, 1500, n_days)\n",
    "    seasonality = 200 * np.sin(2 * np.pi * np.arange(n_days) / 365.25)\n",
    "    weekly_pattern = 100 * np.sin(2 * np.pi * np.arange(n_days) / 7)\n",
    "    noise = np.random.normal(0, 50, n_days)\n",
    "    \n",
    "    sales_data = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'sales': trend + seasonality + weekly_pattern + noise,\n",
    "        'region': np.random.choice(['North', 'South', 'East', 'West'], n_days),\n",
    "        'product': np.random.choice(['Product A', 'Product B', 'Product C', 'Product D'], n_days),\n",
    "        'sales_rep': np.random.choice([f'Rep {i}' for i in range(1, 11)], n_days)\n",
    "    })\n",
    "    sales_data['sales'] = np.maximum(sales_data['sales'], 100)\n",
    "    return sales_data\n",
    "\n",
    "# Load data\n",
    "df = load_sales_data()\n",
    "\n",
    "# Header\n",
    "st.markdown('<h1 class=\"main-header\">ğŸ“Š Sales Analytics Dashboard</h1>', unsafe_allow_html=True)\n",
    "\n",
    "# Sidebar filters\n",
    "st.sidebar.header(\"ğŸ”§ Filters & Controls\")\n",
    "st.sidebar.markdown(\"---\")\n",
    "\n",
    "# Date range selector\n",
    "date_range = st.sidebar.date_input(\n",
    "    \"Select Date Range\",\n",
    "    value=[df['date'].min().date(), df['date'].max().date()],\n",
    "    min_value=df['date'].min().date(),\n",
    "    max_value=df['date'].max().date()\n",
    ")\n",
    "\n",
    "# Region selector\n",
    "regions = st.sidebar.multiselect(\n",
    "    \"Select Regions\",\n",
    "    options=df['region'].unique(),\n",
    "    default=df['region'].unique()\n",
    ")\n",
    "\n",
    "# Product selector\n",
    "products = st.sidebar.multiselect(\n",
    "    \"Select Products\",\n",
    "    options=df['product'].unique(),\n",
    "    default=df['product'].unique()\n",
    ")\n",
    "\n",
    "# Filter data based on selections\n",
    "if len(date_range) == 2:\n",
    "    filtered_df = df[\n",
    "        (df['date'] >= pd.to_datetime(date_range[0])) &\n",
    "        (df['date'] <= pd.to_datetime(date_range[1])) &\n",
    "        (df['region'].isin(regions)) &\n",
    "        (df['product'].isin(products))\n",
    "    ]\n",
    "else:\n",
    "    filtered_df = df[\n",
    "        (df['region'].isin(regions)) &\n",
    "        (df['product'].isin(products))\n",
    "    ]\n",
    "\n",
    "# Key Metrics Row\n",
    "st.subheader(\"ğŸ“ˆ Key Performance Indicators\")\n",
    "col1, col2, col3, col4 = st.columns(4)\n",
    "\n",
    "total_sales = filtered_df['sales'].sum()\n",
    "avg_daily_sales = filtered_df['sales'].mean()\n",
    "max_daily_sales = filtered_df['sales'].max()\n",
    "total_days = len(filtered_df)\n",
    "\n",
    "with col1:\n",
    "    st.metric(\n",
    "        label=\"Total Sales\",\n",
    "        value=f\"${total_sales:,.0f}\",\n",
    "        delta=f\"{total_sales/len(filtered_df)*30:,.0f} (30-day proj.)\"\n",
    "    )\n",
    "\n",
    "with col2:\n",
    "    st.metric(\n",
    "        label=\"Average Daily Sales\", \n",
    "        value=f\"${avg_daily_sales:,.0f}\",\n",
    "        delta=f\"{(avg_daily_sales-1200)/1200*100:+.1f}%\"\n",
    "    )\n",
    "\n",
    "with col3:\n",
    "    st.metric(\n",
    "        label=\"Peak Daily Sales\",\n",
    "        value=f\"${max_daily_sales:,.0f}\",\n",
    "        delta=\"Record high\" if max_daily_sales > 1800 else \"Within range\"\n",
    "    )\n",
    "\n",
    "with col4:\n",
    "    st.metric(\n",
    "        label=\"Days in Period\",\n",
    "        value=f\"{total_days:,}\",\n",
    "        delta=f\"{total_days} days selected\"\n",
    "    )\n",
    "\n",
    "# Charts Row 1\n",
    "st.subheader(\"ğŸ“Š Sales Trends Analysis\")\n",
    "col1, col2 = st.columns([2, 1])\n",
    "\n",
    "with col1:\n",
    "    # Time series chart\n",
    "    daily_sales = filtered_df.groupby('date')['sales'].sum().reset_index()\n",
    "    fig_ts = px.line(\n",
    "        daily_sales, \n",
    "        x='date', \n",
    "        y='sales',\n",
    "        title='Daily Sales Trend',\n",
    "        color_discrete_sequence=['#1f77b4']\n",
    "    )\n",
    "    fig_ts.update_layout(\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=\"Sales ($)\",\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    st.plotly_chart(fig_ts, use_container_width=True)\n",
    "\n",
    "with col2:\n",
    "    # Regional distribution\n",
    "    regional_sales = filtered_df.groupby('region')['sales'].sum().reset_index()\n",
    "    fig_pie = px.pie(\n",
    "        regional_sales,\n",
    "        values='sales',\n",
    "        names='region',\n",
    "        title='Sales by Region'\n",
    "    )\n",
    "    st.plotly_chart(fig_pie, use_container_width=True)\n",
    "\n",
    "# Charts Row 2\n",
    "col1, col2 = st.columns(2)\n",
    "\n",
    "with col1:\n",
    "    # Product performance\n",
    "    product_sales = filtered_df.groupby('product')['sales'].agg(['sum', 'mean', 'count']).reset_index()\n",
    "    product_sales.columns = ['product', 'total_sales', 'avg_sales', 'transaction_count']\n",
    "    \n",
    "    fig_bar = px.bar(\n",
    "        product_sales,\n",
    "        x='product',\n",
    "        y='total_sales',\n",
    "        title='Total Sales by Product',\n",
    "        color='avg_sales',\n",
    "        color_continuous_scale='viridis'\n",
    "    )\n",
    "    st.plotly_chart(fig_bar, use_container_width=True)\n",
    "\n",
    "with col2:\n",
    "    # Sales rep performance\n",
    "    rep_performance = filtered_df.groupby('sales_rep')['sales'].agg(['sum', 'count']).reset_index()\n",
    "    rep_performance.columns = ['sales_rep', 'total_sales', 'transaction_count']\n",
    "    rep_performance = rep_performance.sort_values('total_sales', ascending=True).tail(10)\n",
    "    \n",
    "    fig_horizontal = px.bar(\n",
    "        rep_performance,\n",
    "        x='total_sales',\n",
    "        y='sales_rep',\n",
    "        orientation='h',\n",
    "        title='Top 10 Sales Representatives',\n",
    "        color='total_sales',\n",
    "        color_continuous_scale='plasma'\n",
    "    )\n",
    "    st.plotly_chart(fig_horizontal, use_container_width=True)\n",
    "\n",
    "# Advanced Analytics Section\n",
    "st.subheader(\"ğŸ§ª Advanced Analytics\")\n",
    "\n",
    "# Moving averages\n",
    "st.subheader(\"ğŸ“ˆ Moving Average Analysis\")\n",
    "ma_days = st.slider(\"Moving Average Days\", min_value=7, max_value=60, value=30)\n",
    "\n",
    "daily_sales_ma = daily_sales.copy()\n",
    "daily_sales_ma[f'MA_{ma_days}'] = daily_sales_ma['sales'].rolling(window=ma_days).mean()\n",
    "\n",
    "fig_ma = go.Figure()\n",
    "fig_ma.add_trace(go.Scatter(\n",
    "    x=daily_sales_ma['date'],\n",
    "    y=daily_sales_ma['sales'],\n",
    "    mode='lines',\n",
    "    name='Daily Sales',\n",
    "    line=dict(color='lightblue', width=1),\n",
    "    opacity=0.7\n",
    "))\n",
    "fig_ma.add_trace(go.Scatter(\n",
    "    x=daily_sales_ma['date'],\n",
    "    y=daily_sales_ma[f'MA_{ma_days}'],\n",
    "    mode='lines',\n",
    "    name=f'{ma_days}-Day Moving Average',\n",
    "    line=dict(color='red', width=3)\n",
    "))\n",
    "fig_ma.update_layout(\n",
    "    title=f'Sales Trend with {ma_days}-Day Moving Average',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Sales ($)'\n",
    ")\n",
    "st.plotly_chart(fig_ma, use_container_width=True)\n",
    "\n",
    "# Data Table\n",
    "st.subheader(\"ğŸ“‹ Detailed Data\")\n",
    "if st.checkbox(\"Show raw data\"):\n",
    "    st.dataframe(\n",
    "        filtered_df.head(100),\n",
    "        use_container_width=True\n",
    "    )\n",
    "\n",
    "# Summary statistics\n",
    "st.subheader(\"ğŸ“Š Summary Statistics\")\n",
    "col1, col2 = st.columns(2)\n",
    "\n",
    "with col1:\n",
    "    st.write(\"**Sales Statistics**\")\n",
    "    st.write(filtered_df['sales'].describe())\n",
    "\n",
    "with col2:\n",
    "    st.write(\"**Regional Breakdown**\")\n",
    "    regional_stats = filtered_df.groupby('region')['sales'].agg(['count', 'mean', 'std']).round(2)\n",
    "    st.write(regional_stats)\n",
    "\n",
    "# Footer\n",
    "st.markdown(\"---\")\n",
    "st.markdown(\"**Dashboard Created with Streamlit** | Last Updated: \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "'''\n",
    "\n",
    "# Save the sales dashboard\n",
    "with open(f\"{dashboard_dir}/sales_dashboard.py\", \"w\") as f:\n",
    "    f.write(sales_dashboard_code)\n",
    "\n",
    "print(\"ğŸ“Š Sales Analytics Dashboard Created!\")\n",
    "print(f\"   ğŸ“ File: {dashboard_dir}/sales_dashboard.py\")\n",
    "print(\"   ğŸš€ To run: streamlit run sales_dashboard.py\")\n",
    "print()\n",
    "\n",
    "# Dashboard 2: Scientific Data Explorer\n",
    "scientific_dashboard_code = '''\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.datasets import load_iris, load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"Scientific Data Explorer\",\n",
    "    page_icon=\"ğŸ”¬\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "st.title(\"ğŸ”¬ Scientific Data Explorer\")\n",
    "st.markdown(\"**Interactive analysis of scientific datasets with machine learning insights**\")\n",
    "\n",
    "# Sidebar for dataset selection\n",
    "st.sidebar.header(\"ğŸ§ª Dataset Selection\")\n",
    "dataset_choice = st.sidebar.selectbox(\n",
    "    \"Choose a dataset:\",\n",
    "    [\"Iris Flower Dataset\", \"Wine Quality Dataset\"]\n",
    ")\n",
    "\n",
    "@st.cache_data\n",
    "def load_scientific_data(dataset_name):\n",
    "    if dataset_name == \"Iris Flower Dataset\":\n",
    "        data = load_iris()\n",
    "        df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "        df['target'] = data.target\n",
    "        df['species'] = [data.target_names[i] for i in data.target]\n",
    "        return df, data.feature_names, 'species'\n",
    "    else:  # Wine dataset\n",
    "        data = load_wine()\n",
    "        df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "        df['target'] = data.target\n",
    "        df['wine_class'] = [f'Class {i}' for i in data.target]\n",
    "        return df, data.feature_names, 'wine_class'\n",
    "\n",
    "df, feature_names, target_col = load_scientific_data(dataset_choice)\n",
    "\n",
    "# Analysis options\n",
    "st.sidebar.header(\"ğŸ” Analysis Options\")\n",
    "analysis_type = st.sidebar.selectbox(\n",
    "    \"Select Analysis Type:\",\n",
    "    [\"Exploratory Data Analysis\", \"Principal Component Analysis\", \"Clustering Analysis\", \"Feature Relationships\"]\n",
    ")\n",
    "\n",
    "if analysis_type == \"Exploratory Data Analysis\":\n",
    "    st.header(\"ğŸ“Š Exploratory Data Analysis\")\n",
    "    \n",
    "    # Dataset overview\n",
    "    col1, col2, col3 = st.columns(3)\n",
    "    with col1:\n",
    "        st.metric(\"Total Samples\", len(df))\n",
    "    with col2:\n",
    "        st.metric(\"Features\", len(feature_names))\n",
    "    with col3:\n",
    "        st.metric(\"Classes\", df[target_col].nunique())\n",
    "    \n",
    "    # Feature selection for visualization\n",
    "    selected_features = st.multiselect(\n",
    "        \"Select features to analyze:\",\n",
    "        feature_names,\n",
    "        default=feature_names[:4] if len(feature_names) >= 4 else feature_names\n",
    "    )\n",
    "    \n",
    "    if selected_features:\n",
    "        # Distribution plots\n",
    "        st.subheader(\"ğŸ“ˆ Feature Distributions\")\n",
    "        fig_dist = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=selected_features[:4],\n",
    "            specs=[[{\"type\": \"xy\"}, {\"type\": \"xy\"}],\n",
    "                   [{\"type\": \"xy\"}, {\"type\": \"xy\"}]]\n",
    "        )\n",
    "        \n",
    "        for i, feature in enumerate(selected_features[:4]):\n",
    "            row = i // 2 + 1\n",
    "            col = i % 2 + 1\n",
    "            \n",
    "            for class_name in df[target_col].unique():\n",
    "                class_data = df[df[target_col] == class_name][feature]\n",
    "                fig_dist.add_trace(\n",
    "                    go.Histogram(\n",
    "                        x=class_data,\n",
    "                        name=f\"{class_name}\",\n",
    "                        opacity=0.7,\n",
    "                        legendgroup=class_name,\n",
    "                        showlegend=(i == 0)\n",
    "                    ),\n",
    "                    row=row, col=col\n",
    "                )\n",
    "        \n",
    "        fig_dist.update_layout(\n",
    "            title=\"Feature Distributions by Class\",\n",
    "            height=600,\n",
    "            barmode='overlay'\n",
    "        )\n",
    "        st.plotly_chart(fig_dist, use_container_width=True)\n",
    "        \n",
    "        # Correlation matrix\n",
    "        st.subheader(\"ğŸ”— Feature Correlation Matrix\")\n",
    "        corr_matrix = df[selected_features].corr()\n",
    "        \n",
    "        fig_corr = px.imshow(\n",
    "            corr_matrix,\n",
    "            text_auto=True,\n",
    "            aspect=\"auto\",\n",
    "            title=\"Feature Correlation Heatmap\",\n",
    "            color_continuous_scale=\"RdBu\"\n",
    "        )\n",
    "        st.plotly_chart(fig_corr, use_container_width=True)\n",
    "\n",
    "elif analysis_type == \"Principal Component Analysis\":\n",
    "    st.header(\"ğŸ¯ Principal Component Analysis\")\n",
    "    \n",
    "    # PCA computation\n",
    "    X = df[feature_names]\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    n_components = st.slider(\"Number of PCA Components\", 2, min(len(feature_names), 10), 3)\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # Explained variance\n",
    "    st.subheader(\"ğŸ“Š Explained Variance\")\n",
    "    variance_df = pd.DataFrame({\n",
    "        'Component': [f'PC{i+1}' for i in range(n_components)],\n",
    "        'Explained_Variance': pca.explained_variance_ratio_,\n",
    "        'Cumulative_Variance': np.cumsum(pca.explained_variance_ratio_)\n",
    "    })\n",
    "    \n",
    "    fig_var = px.bar(\n",
    "        variance_df,\n",
    "        x='Component',\n",
    "        y='Explained_Variance',\n",
    "        title='Explained Variance by Principal Component'\n",
    "    )\n",
    "    st.plotly_chart(fig_var, use_container_width=True)\n",
    "    \n",
    "    # PCA scatter plot\n",
    "    st.subheader(\"ğŸ¨ PCA Visualization\")\n",
    "    if n_components >= 2:\n",
    "        pca_df = pd.DataFrame(X_pca[:, :3], columns=[f'PC{i+1}' for i in range(min(3, n_components))])\n",
    "        pca_df[target_col] = df[target_col]\n",
    "        \n",
    "        if n_components >= 3:\n",
    "            fig_3d = px.scatter_3d(\n",
    "                pca_df,\n",
    "                x='PC1',\n",
    "                y='PC2',\n",
    "                z='PC3',\n",
    "                color=target_col,\n",
    "                title='3D PCA Visualization'\n",
    "            )\n",
    "            st.plotly_chart(fig_3d, use_container_width=True)\n",
    "        \n",
    "        fig_2d = px.scatter(\n",
    "            pca_df,\n",
    "            x='PC1',\n",
    "            y='PC2',\n",
    "            color=target_col,\n",
    "            title='2D PCA Visualization'\n",
    "        )\n",
    "        st.plotly_chart(fig_2d, use_container_width=True)\n",
    "\n",
    "elif analysis_type == \"Clustering Analysis\":\n",
    "    st.header(\"ğŸ¯ K-Means Clustering Analysis\")\n",
    "    \n",
    "    # Feature selection for clustering\n",
    "    clustering_features = st.multiselect(\n",
    "        \"Select features for clustering:\",\n",
    "        feature_names,\n",
    "        default=feature_names[:4] if len(feature_names) >= 4 else feature_names\n",
    "    )\n",
    "    \n",
    "    if clustering_features:\n",
    "        X_cluster = df[clustering_features]\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_cluster)\n",
    "        \n",
    "        # Number of clusters\n",
    "        n_clusters = st.slider(\"Number of Clusters\", 2, 10, 3)\n",
    "        \n",
    "        # Perform clustering\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "        \n",
    "        # Add cluster labels to dataframe\n",
    "        df_clustered = df.copy()\n",
    "        df_clustered['Cluster'] = [f'Cluster {i}' for i in cluster_labels]\n",
    "        \n",
    "        # Visualization\n",
    "        if len(clustering_features) >= 2:\n",
    "            fig_cluster = px.scatter(\n",
    "                df_clustered,\n",
    "                x=clustering_features[0],\n",
    "                y=clustering_features[1],\n",
    "                color='Cluster',\n",
    "                symbol=target_col,\n",
    "                title=f'K-Means Clustering (k={n_clusters})',\n",
    "                hover_data=clustering_features[2:] if len(clustering_features) > 2 else None\n",
    "            )\n",
    "            st.plotly_chart(fig_cluster, use_container_width=True)\n",
    "        \n",
    "        # Cluster centers\n",
    "        st.subheader(\"ğŸ¯ Cluster Centers\")\n",
    "        centers_df = pd.DataFrame(\n",
    "            scaler.inverse_transform(kmeans.cluster_centers_),\n",
    "            columns=clustering_features,\n",
    "            index=[f'Cluster {i}' for i in range(n_clusters)]\n",
    "        )\n",
    "        st.dataframe(centers_df, use_container_width=True)\n",
    "\n",
    "else:  # Feature Relationships\n",
    "    st.header(\"ğŸ”— Feature Relationships Analysis\")\n",
    "    \n",
    "    # Feature selection\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        x_feature = st.selectbox(\"Select X-axis feature:\", feature_names)\n",
    "    with col2:\n",
    "        y_feature = st.selectbox(\"Select Y-axis feature:\", feature_names, index=1)\n",
    "    \n",
    "    # Scatter plot with regression\n",
    "    fig_scatter = px.scatter(\n",
    "        df,\n",
    "        x=x_feature,\n",
    "        y=y_feature,\n",
    "        color=target_col,\n",
    "        trendline=\"ols\",\n",
    "        title=f'{x_feature} vs {y_feature}',\n",
    "        hover_data=feature_names\n",
    "    )\n",
    "    st.plotly_chart(fig_scatter, use_container_width=True)\n",
    "    \n",
    "    # Statistical summary\n",
    "    st.subheader(\"ğŸ“Š Statistical Summary\")\n",
    "    correlation = df[x_feature].corr(df[y_feature])\n",
    "    st.metric(\"Pearson Correlation\", f\"{correlation:.3f}\")\n",
    "    \n",
    "    # Box plots\n",
    "    st.subheader(\"ğŸ“¦ Distribution by Class\")\n",
    "    col1, col2 = st.columns(2)\n",
    "    \n",
    "    with col1:\n",
    "        fig_box1 = px.box(df, x=target_col, y=x_feature, title=f'{x_feature} by Class')\n",
    "        st.plotly_chart(fig_box1, use_container_width=True)\n",
    "    \n",
    "    with col2:\n",
    "        fig_box2 = px.box(df, x=target_col, y=y_feature, title=f'{y_feature} by Class')\n",
    "        st.plotly_chart(fig_box2, use_container_width=True)\n",
    "\n",
    "# Raw data display\n",
    "st.sidebar.markdown(\"---\")\n",
    "if st.sidebar.checkbox(\"Show Raw Data\"):\n",
    "    st.subheader(\"ğŸ“‹ Raw Dataset\")\n",
    "    st.dataframe(df, use_container_width=True)\n",
    "\n",
    "st.markdown(\"---\")\n",
    "st.markdown(\"**Scientific Data Explorer** | Built with Streamlit & Scikit-learn\")\n",
    "'''\n",
    "\n",
    "# Save the scientific dashboard\n",
    "with open(f\"{dashboard_dir}/scientific_explorer.py\", \"w\") as f:\n",
    "    f.write(scientific_dashboard_code)\n",
    "\n",
    "print(\"ğŸ”¬ Scientific Data Explorer Created!\")\n",
    "print(f\"   ğŸ“ File: {dashboard_dir}/scientific_explorer.py\")\n",
    "print(\"   ğŸš€ To run: streamlit run scientific_explorer.py\")\n",
    "print()\n",
    "\n",
    "# Dashboard 3: Multi-Page Dashboard Template\n",
    "multipage_dashboard_code = '''\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Configure the page\n",
    "st.set_page_config(\n",
    "    page_title=\"Multi-Page Dashboard\",\n",
    "    page_icon=\"ğŸ“š\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\"\n",
    ")\n",
    "\n",
    "# Initialize session state\n",
    "if 'page' not in st.session_state:\n",
    "    st.session_state.page = 'Overview'\n",
    "\n",
    "# Navigation\n",
    "st.sidebar.title(\"ğŸ“š Navigation\")\n",
    "pages = {\n",
    "    \"ğŸ“Š Overview\": \"Overview\",\n",
    "    \"ğŸ“ˆ Analytics\": \"Analytics\", \n",
    "    \"ğŸ¯ Predictions\": \"Predictions\",\n",
    "    \"âš™ï¸ Settings\": \"Settings\"\n",
    "}\n",
    "\n",
    "selected_page = st.sidebar.radio(\"Go to:\", list(pages.keys()))\n",
    "st.session_state.page = pages[selected_page]\n",
    "\n",
    "# Sample data\n",
    "@st.cache_data\n",
    "def generate_sample_data():\n",
    "    np.random.seed(42)\n",
    "    dates = pd.date_range('2024-01-01', periods=365, freq='D')\n",
    "    data = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'metric_a': np.cumsum(np.random.randn(365)) + 100,\n",
    "        'metric_b': np.cumsum(np.random.randn(365)) + 50,\n",
    "        'category': np.random.choice(['X', 'Y', 'Z'], 365),\n",
    "        'region': np.random.choice(['North', 'South', 'East', 'West'], 365)\n",
    "    })\n",
    "    return data\n",
    "\n",
    "df = generate_sample_data()\n",
    "\n",
    "# Page content\n",
    "if st.session_state.page == \"Overview\":\n",
    "    st.title(\"ğŸ“Š Dashboard Overview\")\n",
    "    st.markdown(\"Welcome to the multi-page interactive dashboard!\")\n",
    "    \n",
    "    # Key metrics\n",
    "    col1, col2, col3, col4 = st.columns(4)\n",
    "    with col1:\n",
    "        st.metric(\"Total Records\", len(df), delta=\"365 days\")\n",
    "    with col2:\n",
    "        st.metric(\"Avg Metric A\", f\"{df['metric_a'].mean():.1f}\", delta=\"â†‘ 2.3%\")\n",
    "    with col3:\n",
    "        st.metric(\"Avg Metric B\", f\"{df['metric_b'].mean():.1f}\", delta=\"â†“ 1.1%\")\n",
    "    with col4:\n",
    "        st.metric(\"Categories\", df['category'].nunique(), delta=\"3 active\")\n",
    "    \n",
    "    # Overview charts\n",
    "    col1, col2 = st.columns(2)\n",
    "    \n",
    "    with col1:\n",
    "        fig1 = px.line(df, x='date', y='metric_a', title='Metric A Trend')\n",
    "        st.plotly_chart(fig1, use_container_width=True)\n",
    "    \n",
    "    with col2:\n",
    "        category_counts = df['category'].value_counts()\n",
    "        fig2 = px.pie(values=category_counts.values, names=category_counts.index, title='Category Distribution')\n",
    "        st.plotly_chart(fig2, use_container_width=True)\n",
    "\n",
    "elif st.session_state.page == \"Analytics\":\n",
    "    st.title(\"ğŸ“ˆ Advanced Analytics\")\n",
    "    \n",
    "    # Filters\n",
    "    st.sidebar.header(\"ğŸ”§ Filters\")\n",
    "    date_range = st.sidebar.date_input(\n",
    "        \"Date Range\",\n",
    "        value=[df['date'].min().date(), df['date'].max().date()],\n",
    "        min_value=df['date'].min().date(),\n",
    "        max_value=df['date'].max().date()\n",
    "    )\n",
    "    \n",
    "    selected_categories = st.sidebar.multiselect(\n",
    "        \"Categories\",\n",
    "        df['category'].unique(),\n",
    "        default=df['category'].unique()\n",
    "    )\n",
    "    \n",
    "    # Filter data\n",
    "    if len(date_range) == 2:\n",
    "        filtered_df = df[\n",
    "            (df['date'] >= pd.to_datetime(date_range[0])) &\n",
    "            (df['date'] <= pd.to_datetime(date_range[1])) &\n",
    "            (df['category'].isin(selected_categories))\n",
    "        ]\n",
    "    else:\n",
    "        filtered_df = df[df['category'].isin(selected_categories)]\n",
    "    \n",
    "    # Analytics content\n",
    "    tab1, tab2, tab3 = st.tabs([\"ğŸ“Š Trends\", \"ğŸ” Correlations\", \"ğŸ“‹ Statistics\"])\n",
    "    \n",
    "    with tab1:\n",
    "        fig = px.line(filtered_df, x='date', y=['metric_a', 'metric_b'], title='Metrics Over Time')\n",
    "        st.plotly_chart(fig, use_container_width=True)\n",
    "        \n",
    "        # Regional analysis\n",
    "        regional_data = filtered_df.groupby(['region', 'category']).agg({\n",
    "            'metric_a': 'mean',\n",
    "            'metric_b': 'mean'\n",
    "        }).reset_index()\n",
    "        \n",
    "        fig_region = px.bar(\n",
    "            regional_data, \n",
    "            x='region', \n",
    "            y='metric_a', \n",
    "            color='category',\n",
    "            title='Average Metric A by Region and Category'\n",
    "        )\n",
    "        st.plotly_chart(fig_region, use_container_width=True)\n",
    "    \n",
    "    with tab2:\n",
    "        # Correlation analysis\n",
    "        correlation = filtered_df[['metric_a', 'metric_b']].corr()\n",
    "        fig_corr = px.imshow(correlation, text_auto=True, title='Metric Correlation')\n",
    "        st.plotly_chart(fig_corr, use_container_width=True)\n",
    "        \n",
    "        # Scatter plot\n",
    "        fig_scatter = px.scatter(\n",
    "            filtered_df, \n",
    "            x='metric_a', \n",
    "            y='metric_b', \n",
    "            color='category',\n",
    "            title='Metric A vs Metric B'\n",
    "        )\n",
    "        st.plotly_chart(fig_scatter, use_container_width=True)\n",
    "    \n",
    "    with tab3:\n",
    "        st.subheader(\"ğŸ“Š Summary Statistics\")\n",
    "        st.dataframe(filtered_df[['metric_a', 'metric_b']].describe())\n",
    "        \n",
    "        st.subheader(\"ğŸ“‹ Sample Data\")\n",
    "        st.dataframe(filtered_df.head(20))\n",
    "\n",
    "elif st.session_state.page == \"Predictions\":\n",
    "    st.title(\"ğŸ¯ Predictive Analytics\")\n",
    "    st.info(\"This section would contain machine learning models and predictions.\")\n",
    "    \n",
    "    # Simple moving average prediction\n",
    "    window = st.slider(\"Moving Average Window\", 5, 50, 20)\n",
    "    \n",
    "    df_pred = df.copy()\n",
    "    df_pred['ma_metric_a'] = df_pred['metric_a'].rolling(window=window).mean()\n",
    "    \n",
    "    # Simple linear extrapolation for demo\n",
    "    last_values = df_pred['metric_a'].tail(window).values\n",
    "    trend = np.polyfit(range(window), last_values, 1)\n",
    "    future_dates = pd.date_range(df['date'].max() + timedelta(days=1), periods=30, freq='D')\n",
    "    future_values = [trend[0] * (window + i) + trend[1] for i in range(30)]\n",
    "    \n",
    "    # Combine historical and predicted\n",
    "    pred_df = pd.DataFrame({\n",
    "        'date': list(df['date']) + list(future_dates),\n",
    "        'metric_a': list(df['metric_a']) + [np.nan] * 30,\n",
    "        'predicted': [np.nan] * len(df) + future_values\n",
    "    })\n",
    "    \n",
    "    fig_pred = go.Figure()\n",
    "    fig_pred.add_trace(go.Scatter(\n",
    "        x=pred_df['date'][:len(df)],\n",
    "        y=pred_df['metric_a'][:len(df)],\n",
    "        mode='lines',\n",
    "        name='Historical',\n",
    "        line=dict(color='blue')\n",
    "    ))\n",
    "    fig_pred.add_trace(go.Scatter(\n",
    "        x=pred_df['date'][len(df):],\n",
    "        y=pred_df['predicted'][len(df):],\n",
    "        mode='lines',\n",
    "        name='Predicted',\n",
    "        line=dict(color='red', dash='dash')\n",
    "    ))\n",
    "    fig_pred.update_layout(title='Metric A: Historical vs Predicted')\n",
    "    st.plotly_chart(fig_pred, use_container_width=True)\n",
    "\n",
    "else:  # Settings\n",
    "    st.title(\"âš™ï¸ Dashboard Settings\")\n",
    "    \n",
    "    # Theme settings\n",
    "    st.subheader(\"ğŸ¨ Appearance\")\n",
    "    theme_color = st.color_picker(\"Primary Color\", \"#1f77b4\")\n",
    "    \n",
    "    # Data settings\n",
    "    st.subheader(\"ğŸ“Š Data Configuration\")\n",
    "    refresh_rate = st.selectbox(\"Data Refresh Rate\", [\"Manual\", \"5 minutes\", \"15 minutes\", \"1 hour\"])\n",
    "    \n",
    "    # Export settings\n",
    "    st.subheader(\"ğŸ“¤ Export Options\")\n",
    "    export_format = st.selectbox(\"Default Export Format\", [\"CSV\", \"Excel\", \"JSON\"])\n",
    "    \n",
    "    # Save settings\n",
    "    if st.button(\"Save Settings\"):\n",
    "        st.success(\"Settings saved successfully!\")\n",
    "        \n",
    "    # Reset button\n",
    "    if st.button(\"Reset to Defaults\"):\n",
    "        st.warning(\"Settings reset to defaults.\")\n",
    "\n",
    "# Footer\n",
    "st.sidebar.markdown(\"---\")\n",
    "st.sidebar.markdown(\"**Multi-Page Dashboard** | Built with Streamlit\")\n",
    "st.sidebar.markdown(f\"Current time: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "'''\n",
    "\n",
    "# Save the multi-page dashboard\n",
    "with open(f\"{dashboard_dir}/multipage_dashboard.py\", \"w\") as f:\n",
    "    f.write(multipage_dashboard_code)\n",
    "\n",
    "print(\"ğŸ“š Multi-Page Dashboard Template Created!\")\n",
    "print(f\"   ğŸ“ File: {dashboard_dir}/multipage_dashboard.py\")\n",
    "print(\"   ğŸš€ To run: streamlit run multipage_dashboard.py\")\n",
    "print()\n",
    "\n",
    "print(\"âœ… All Dashboard Examples Created Successfully!\")\n",
    "print(f\"ğŸ“ Dashboard files location: {dashboard_dir}\")\n",
    "print(\"\\nğŸš€ To run any dashboard:\")\n",
    "print(\"   1. Open terminal in the dashboard directory\")\n",
    "print(\"   2. Run: streamlit run <dashboard_name>.py\")\n",
    "print(\"   3. Dashboard will open in your web browser\")\n",
    "print(\"\\nğŸ“‹ Available Dashboards:\")\n",
    "print(\"   â€¢ sales_dashboard.py - Sales analytics with filters and KPIs\")\n",
    "print(\"   â€¢ scientific_explorer.py - Scientific data analysis with ML\")\n",
    "print(\"   â€¢ multipage_dashboard.py - Multi-page app template\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cc4968",
   "metadata": {},
   "source": [
    "## âš¡ Performance Optimization & Deployment\n",
    "\n",
    "Let's explore advanced Streamlit techniques for optimizing dashboard performance and preparing for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0db396bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš¡ STREAMLIT PERFORMANCE OPTIMIZATION TECHNIQUES\n",
      "============================================================\n",
      "ğŸ“‹ Caching Best Practices:\n",
      "âœ… Use @st.cache_data for data loading and transformations\n",
      "âœ… Use @st.cache_resource for ML models and database connections\n",
      "âœ… Use st.session_state for user interaction state\n",
      "âœ… Cache expensive computations (PCA, clustering, etc.)\n",
      "âŒ Don't cache Streamlit widgets or UI elements\n",
      "\n",
      "âš¡ Optimized Performance Dashboard Created!\n",
      "   ğŸ“ File: /Users/sanjeevadodlapati/Downloads/Repos/DataVisualization-Comprehensive-Tutorial/outputs/dashboards/optimized_dashboard.py\n",
      "   ğŸš€ Features: Caching, session state, performance monitoring\n",
      "\n",
      "ğŸš€ DEPLOYMENT CONFIGURATIONS\n",
      "========================================\n",
      "ğŸ“¦ Deployment Files Created:\n",
      "   âœ… .streamlit/config.toml - Streamlit configuration\n",
      "   âœ… requirements.txt - Python dependencies\n",
      "   âœ… deploy.sh - Deployment script\n",
      "\n",
      "ğŸŒ DEPLOYMENT OPTIONS:\n",
      "==============================\n",
      "1. ğŸ–¥ï¸  Local Development:\n",
      "   streamlit run dashboard_name.py\n",
      "\n",
      "2. â˜ï¸  Streamlit Cloud:\n",
      "   â€¢ Push to GitHub repository\n",
      "   â€¢ Connect at share.streamlit.io\n",
      "   â€¢ Automatic deployment from main branch\n",
      "\n",
      "3. ğŸ³ Docker Deployment:\n",
      "   â€¢ Create Dockerfile with Streamlit\n",
      "   â€¢ Deploy to cloud platforms (AWS, GCP, Azure)\n",
      "\n",
      "4. ğŸš€ Advanced Hosting:\n",
      "   â€¢ Heroku with Procfile\n",
      "   â€¢ AWS EC2 with reverse proxy\n",
      "   â€¢ Kubernetes cluster deployment\n",
      "\n",
      "âš¡ PERFORMANCE OPTIMIZATION SUMMARY:\n",
      "=============================================\n",
      "   âœ… Use @st.cache_data for data loading and transformations\n",
      "   âœ… Use @st.cache_resource for ML models and connections\n",
      "   âœ… Implement session state for user interactions\n",
      "   âœ… Use st.columns() for efficient layouts\n",
      "   âœ… Add loading spinners for better UX\n",
      "   âœ… Monitor performance with timing metrics\n",
      "   âœ… Clear cache periodically for memory management\n",
      "   âœ… Use lazy loading for large datasets\n",
      "   âœ… Optimize Plotly charts with sampling for large data\n",
      "   âœ… Configure page layout for better performance\n",
      "\n",
      "ğŸ¯ Ready for production-grade dashboard deployment!\n"
     ]
    }
   ],
   "source": [
    "# Performance Optimization Techniques for Streamlit\n",
    "\n",
    "# 1. Demonstrate caching strategies\n",
    "print(\"âš¡ STREAMLIT PERFORMANCE OPTIMIZATION TECHNIQUES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Example: Advanced caching with @st.cache_data\n",
    "caching_example = '''\n",
    "# âœ… GOOD: Efficient data loading with caching\n",
    "@st.cache_data\n",
    "def load_large_dataset(file_path, filters=None):\n",
    "    \"\"\"Cached function for loading and filtering data\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    if filters:\n",
    "        for column, values in filters.items():\n",
    "            df = df[df[column].isin(values)]\n",
    "    return df\n",
    "\n",
    "# âœ… GOOD: Cached expensive computations  \n",
    "@st.cache_data\n",
    "def compute_complex_analysis(data, analysis_type):\n",
    "    \"\"\"Cache expensive calculations\"\"\"\n",
    "    if analysis_type == \"pca\":\n",
    "        from sklearn.decomposition import PCA\n",
    "        pca = PCA(n_components=2)\n",
    "        return pca.fit_transform(data)\n",
    "    elif analysis_type == \"clustering\":\n",
    "        from sklearn.cluster import KMeans\n",
    "        kmeans = KMeans(n_clusters=3)\n",
    "        return kmeans.fit_predict(data)\n",
    "\n",
    "# âœ… GOOD: Session state for user interactions\n",
    "if 'user_selections' not in st.session_state:\n",
    "    st.session_state.user_selections = {}\n",
    "\n",
    "# âŒ BAD: Loading data without caching\n",
    "# df = pd.read_csv(\"large_file.csv\")  # Runs every time!\n",
    "\n",
    "# âŒ BAD: Complex computations without caching\n",
    "# pca_result = PCA().fit_transform(data)  # Recalculates every interaction!\n",
    "'''\n",
    "\n",
    "print(\"ğŸ“‹ Caching Best Practices:\")\n",
    "print(\"âœ… Use @st.cache_data for data loading and transformations\")\n",
    "print(\"âœ… Use @st.cache_resource for ML models and database connections\")\n",
    "print(\"âœ… Use st.session_state for user interaction state\")\n",
    "print(\"âœ… Cache expensive computations (PCA, clustering, etc.)\")\n",
    "print(\"âŒ Don't cache Streamlit widgets or UI elements\")\n",
    "print()\n",
    "\n",
    "# 2. Create optimized dashboard example\n",
    "optimized_dashboard_code = '''\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "\n",
    "# Configure page with performance settings\n",
    "st.set_page_config(\n",
    "    page_title=\"Optimized Dashboard\",\n",
    "    page_icon=\"âš¡\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\"\n",
    ")\n",
    "\n",
    "# Performance monitoring\n",
    "@st.cache_data\n",
    "def generate_large_dataset(n_samples=10000):\n",
    "    \"\"\"Generate large dataset with caching\"\"\"\n",
    "    X, y = make_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=20,\n",
    "        n_informative=15,\n",
    "        n_redundant=5,\n",
    "        n_clusters_per_class=1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    feature_names = [f'feature_{i}' for i in range(20)]\n",
    "    df = pd.DataFrame(X, columns=feature_names)\n",
    "    df['target'] = y\n",
    "    df['category'] = np.random.choice(['A', 'B', 'C'], n_samples)\n",
    "    \n",
    "    return df\n",
    "\n",
    "@st.cache_data\n",
    "def compute_pca_analysis(data, n_components=2):\n",
    "    \"\"\"Cached PCA computation\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "    \n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_result = pca.fit_transform(data_scaled)\n",
    "    \n",
    "    return pca_result, pca.explained_variance_ratio_\n",
    "\n",
    "@st.cache_data\n",
    "def filter_data(df, selected_categories, feature_range):\n",
    "    \"\"\"Cached data filtering\"\"\"\n",
    "    filtered_df = df[df['category'].isin(selected_categories)]\n",
    "    \n",
    "    # Apply feature range filtering if provided\n",
    "    if feature_range:\n",
    "        for feature, (min_val, max_val) in feature_range.items():\n",
    "            filtered_df = filtered_df[\n",
    "                (filtered_df[feature] >= min_val) & \n",
    "                (filtered_df[feature] <= max_val)\n",
    "            ]\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "# Initialize session state\n",
    "if 'data_loaded' not in st.session_state:\n",
    "    st.session_state.data_loaded = False\n",
    "if 'analysis_cache' not in st.session_state:\n",
    "    st.session_state.analysis_cache = {}\n",
    "\n",
    "# Title and description\n",
    "st.title(\"âš¡ High-Performance Dashboard\")\n",
    "st.markdown(\"**Demonstration of optimized Streamlit techniques for large datasets**\")\n",
    "\n",
    "# Performance metrics in sidebar\n",
    "st.sidebar.header(\"ğŸ“Š Performance Metrics\")\n",
    "\n",
    "# Data loading with progress\n",
    "if not st.session_state.data_loaded:\n",
    "    with st.spinner(\"Loading dataset...\"):\n",
    "        start_time = time.time()\n",
    "        df = generate_large_dataset(10000)\n",
    "        load_time = time.time() - start_time\n",
    "        st.session_state.data_loaded = True\n",
    "        st.session_state.df = df\n",
    "        st.sidebar.success(f\"Data loaded in {load_time:.2f}s\")\n",
    "else:\n",
    "    df = st.session_state.df\n",
    "    st.sidebar.info(\"Data loaded from cache\")\n",
    "\n",
    "# Sidebar controls\n",
    "st.sidebar.header(\"ğŸ”§ Controls\")\n",
    "\n",
    "# Category filter\n",
    "selected_categories = st.sidebar.multiselect(\n",
    "    \"Select Categories\",\n",
    "    options=['A', 'B', 'C'],\n",
    "    default=['A', 'B', 'C'],\n",
    "    key=\"category_filter\"\n",
    ")\n",
    "\n",
    "# Feature selection\n",
    "feature_columns = [col for col in df.columns if col.startswith('feature_')]\n",
    "selected_features = st.sidebar.multiselect(\n",
    "    \"Select Features for Analysis\",\n",
    "    options=feature_columns,\n",
    "    default=feature_columns[:5],\n",
    "    key=\"feature_selection\"\n",
    ")\n",
    "\n",
    "# Analysis type\n",
    "analysis_type = st.sidebar.selectbox(\n",
    "    \"Analysis Type\",\n",
    "    [\"Overview\", \"PCA Analysis\", \"Feature Correlation\"],\n",
    "    key=\"analysis_type\"\n",
    ")\n",
    "\n",
    "# Only filter data when selections change\n",
    "filter_key = f\"{tuple(selected_categories)}_{tuple(selected_features)}\"\n",
    "if filter_key not in st.session_state.analysis_cache:\n",
    "    with st.spinner(\"Filtering data...\"):\n",
    "        start_time = time.time()\n",
    "        filtered_df = filter_data(df, selected_categories, None)\n",
    "        filter_time = time.time() - start_time\n",
    "        st.session_state.analysis_cache[filter_key] = filtered_df\n",
    "        st.sidebar.info(f\"Filter time: {filter_time:.3f}s\")\n",
    "else:\n",
    "    filtered_df = st.session_state.analysis_cache[filter_key]\n",
    "    st.sidebar.info(\"Filtered data from cache\")\n",
    "\n",
    "# Display metrics\n",
    "col1, col2, col3, col4 = st.columns(4)\n",
    "with col1:\n",
    "    st.metric(\"Total Records\", f\"{len(df):,}\")\n",
    "with col2:\n",
    "    st.metric(\"Filtered Records\", f\"{len(filtered_df):,}\")\n",
    "with col3:\n",
    "    st.metric(\"Selected Features\", len(selected_features))\n",
    "with col4:\n",
    "    st.metric(\"Cache Entries\", len(st.session_state.analysis_cache))\n",
    "\n",
    "# Main content based on analysis type\n",
    "if analysis_type == \"Overview\":\n",
    "    st.subheader(\"ğŸ“Š Dataset Overview\")\n",
    "    \n",
    "    # Use columns for efficient layout\n",
    "    col1, col2 = st.columns(2)\n",
    "    \n",
    "    with col1:\n",
    "        # Category distribution\n",
    "        category_counts = filtered_df['category'].value_counts()\n",
    "        fig_pie = px.pie(\n",
    "            values=category_counts.values,\n",
    "            names=category_counts.index,\n",
    "            title=\"Category Distribution\"\n",
    "        )\n",
    "        st.plotly_chart(fig_pie, use_container_width=True, key=\"category_pie\")\n",
    "    \n",
    "    with col2:\n",
    "        # Target distribution\n",
    "        target_counts = filtered_df['target'].value_counts()\n",
    "        fig_bar = px.bar(\n",
    "            x=target_counts.index,\n",
    "            y=target_counts.values,\n",
    "            title=\"Target Distribution\"\n",
    "        )\n",
    "        st.plotly_chart(fig_bar, use_container_width=True, key=\"target_bar\")\n",
    "\n",
    "elif analysis_type == \"PCA Analysis\":\n",
    "    if len(selected_features) >= 2:\n",
    "        st.subheader(\"ğŸ¯ Principal Component Analysis\")\n",
    "        \n",
    "        # Cached PCA computation\n",
    "        pca_key = f\"pca_{filter_key}_{tuple(selected_features)}\"\n",
    "        if pca_key not in st.session_state.analysis_cache:\n",
    "            with st.spinner(\"Computing PCA...\"):\n",
    "                start_time = time.time()\n",
    "                pca_result, variance_ratio = compute_pca_analysis(\n",
    "                    filtered_df[selected_features], \n",
    "                    n_components=min(3, len(selected_features))\n",
    "                )\n",
    "                pca_time = time.time() - start_time\n",
    "                st.session_state.analysis_cache[pca_key] = (pca_result, variance_ratio)\n",
    "                st.sidebar.info(f\"PCA time: {pca_time:.3f}s\")\n",
    "        else:\n",
    "            pca_result, variance_ratio = st.session_state.analysis_cache[pca_key]\n",
    "            st.sidebar.info(\"PCA from cache\")\n",
    "        \n",
    "        # Explained variance\n",
    "        col1, col2 = st.columns(2)\n",
    "        \n",
    "        with col1:\n",
    "            variance_df = pd.DataFrame({\n",
    "                'Component': [f'PC{i+1}' for i in range(len(variance_ratio))],\n",
    "                'Explained_Variance': variance_ratio\n",
    "            })\n",
    "            fig_var = px.bar(\n",
    "                variance_df,\n",
    "                x='Component',\n",
    "                y='Explained_Variance',\n",
    "                title='Explained Variance by Component'\n",
    "            )\n",
    "            st.plotly_chart(fig_var, use_container_width=True)\n",
    "        \n",
    "        with col2:\n",
    "            # PCA scatter plot\n",
    "            pca_df = pd.DataFrame(pca_result[:, :2], columns=['PC1', 'PC2'])\n",
    "            pca_df['category'] = filtered_df['category'].values\n",
    "            pca_df['target'] = filtered_df['target'].values\n",
    "            \n",
    "            fig_pca = px.scatter(\n",
    "                pca_df,\n",
    "                x='PC1',\n",
    "                y='PC2',\n",
    "                color='category',\n",
    "                symbol='target',\n",
    "                title='PCA Visualization'\n",
    "            )\n",
    "            st.plotly_chart(fig_pca, use_container_width=True)\n",
    "    else:\n",
    "        st.warning(\"Please select at least 2 features for PCA analysis\")\n",
    "\n",
    "else:  # Feature Correlation\n",
    "    if len(selected_features) >= 2:\n",
    "        st.subheader(\"ğŸ”— Feature Correlation Analysis\")\n",
    "        \n",
    "        # Cached correlation computation\n",
    "        corr_key = f\"corr_{filter_key}_{tuple(selected_features)}\"\n",
    "        if corr_key not in st.session_state.analysis_cache:\n",
    "            with st.spinner(\"Computing correlations...\"):\n",
    "                start_time = time.time()\n",
    "                correlation_matrix = filtered_df[selected_features].corr()\n",
    "                corr_time = time.time() - start_time\n",
    "                st.session_state.analysis_cache[corr_key] = correlation_matrix\n",
    "                st.sidebar.info(f\"Correlation time: {corr_time:.3f}s\")\n",
    "        else:\n",
    "            correlation_matrix = st.session_state.analysis_cache[corr_key]\n",
    "            st.sidebar.info(\"Correlation from cache\")\n",
    "        \n",
    "        # Correlation heatmap\n",
    "        fig_corr = px.imshow(\n",
    "            correlation_matrix,\n",
    "            text_auto=True,\n",
    "            aspect=\"auto\",\n",
    "            title=\"Feature Correlation Matrix\"\n",
    "        )\n",
    "        st.plotly_chart(fig_corr, use_container_width=True)\n",
    "    else:\n",
    "        st.warning(\"Please select at least 2 features for correlation analysis\")\n",
    "\n",
    "# Performance summary\n",
    "st.sidebar.markdown(\"---\")\n",
    "st.sidebar.subheader(\"âš¡ Performance Summary\")\n",
    "st.sidebar.info(f\"Cache size: {len(st.session_state.analysis_cache)} entries\")\n",
    "st.sidebar.info(f\"Total records: {len(df):,}\")\n",
    "\n",
    "# Clear cache button\n",
    "if st.sidebar.button(\"Clear Cache\"):\n",
    "    st.session_state.analysis_cache.clear()\n",
    "    st.sidebar.success(\"Cache cleared!\")\n",
    "'''\n",
    "\n",
    "# Save optimized dashboard\n",
    "with open(f\"{dashboard_dir}/optimized_dashboard.py\", \"w\") as f:\n",
    "    f.write(optimized_dashboard_code)\n",
    "\n",
    "print(\"âš¡ Optimized Performance Dashboard Created!\")\n",
    "print(f\"   ğŸ“ File: {dashboard_dir}/optimized_dashboard.py\")\n",
    "print(\"   ğŸš€ Features: Caching, session state, performance monitoring\")\n",
    "print()\n",
    "\n",
    "# 3. Deployment configurations\n",
    "print(\"ğŸš€ DEPLOYMENT CONFIGURATIONS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create Streamlit config\n",
    "config_toml = '''\n",
    "[global]\n",
    "dataFrameSerialization = \"arrow\"\n",
    "\n",
    "[server]\n",
    "runOnSave = true\n",
    "port = 8501\n",
    "enableCORS = false\n",
    "enableXsrfProtection = false\n",
    "\n",
    "[browser]\n",
    "gatherUsageStats = false\n",
    "\n",
    "[theme]\n",
    "primaryColor = \"#1f77b4\"\n",
    "backgroundColor = \"#ffffff\"\n",
    "secondaryBackgroundColor = \"#f0f2f6\"\n",
    "textColor = \"#262730\"\n",
    "'''\n",
    "\n",
    "# Save config\n",
    "config_dir = f\"{dashboard_dir}/.streamlit\"\n",
    "os.makedirs(config_dir, exist_ok=True)\n",
    "with open(f\"{config_dir}/config.toml\", \"w\") as f:\n",
    "    f.write(config_toml)\n",
    "\n",
    "# Create requirements file\n",
    "requirements_txt = '''\n",
    "streamlit>=1.48.0\n",
    "pandas>=2.0.0\n",
    "numpy>=1.24.0\n",
    "plotly>=5.0.0\n",
    "scikit-learn>=1.3.0\n",
    "'''\n",
    "\n",
    "with open(f\"{dashboard_dir}/requirements.txt\", \"w\") as f:\n",
    "    f.write(requirements_txt)\n",
    "\n",
    "# Create deployment script\n",
    "deploy_script = '''#!/bin/bash\n",
    "# Streamlit Dashboard Deployment Script\n",
    "\n",
    "echo \"ğŸš€ Starting Streamlit Dashboard Deployment\"\n",
    "\n",
    "# Check if virtual environment exists\n",
    "if [ ! -d \"venv\" ]; then\n",
    "    echo \"ğŸ“¦ Creating virtual environment...\"\n",
    "    python -m venv venv\n",
    "fi\n",
    "\n",
    "# Activate virtual environment\n",
    "source venv/bin/activate\n",
    "\n",
    "# Install requirements\n",
    "echo \"ğŸ“¥ Installing dependencies...\"\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# Set environment variables\n",
    "export STREAMLIT_SERVER_ENABLE_CORS=false\n",
    "export STREAMLIT_SERVER_ENABLE_XSRF_PROTECTION=false\n",
    "\n",
    "echo \"âœ… Setup complete!\"\n",
    "echo \"ğŸŒ Available dashboards:\"\n",
    "echo \"   â€¢ streamlit run sales_dashboard.py\"\n",
    "echo \"   â€¢ streamlit run scientific_explorer.py\"\n",
    "echo \"   â€¢ streamlit run multipage_dashboard.py\"\n",
    "echo \"   â€¢ streamlit run optimized_dashboard.py\"\n",
    "'''\n",
    "\n",
    "with open(f\"{dashboard_dir}/deploy.sh\", \"w\") as f:\n",
    "    f.write(deploy_script)\n",
    "\n",
    "# Make script executable\n",
    "os.chmod(f\"{dashboard_dir}/deploy.sh\", 0o755)\n",
    "\n",
    "print(\"ğŸ“¦ Deployment Files Created:\")\n",
    "print(\"   âœ… .streamlit/config.toml - Streamlit configuration\")\n",
    "print(\"   âœ… requirements.txt - Python dependencies\")\n",
    "print(\"   âœ… deploy.sh - Deployment script\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸŒ DEPLOYMENT OPTIONS:\")\n",
    "print(\"=\" * 30)\n",
    "print(\"1. ğŸ–¥ï¸  Local Development:\")\n",
    "print(\"   streamlit run dashboard_name.py\")\n",
    "print()\n",
    "print(\"2. â˜ï¸  Streamlit Cloud:\")\n",
    "print(\"   â€¢ Push to GitHub repository\")\n",
    "print(\"   â€¢ Connect at share.streamlit.io\")\n",
    "print(\"   â€¢ Automatic deployment from main branch\")\n",
    "print()\n",
    "print(\"3. ğŸ³ Docker Deployment:\")\n",
    "print(\"   â€¢ Create Dockerfile with Streamlit\")\n",
    "print(\"   â€¢ Deploy to cloud platforms (AWS, GCP, Azure)\")\n",
    "print()\n",
    "print(\"4. ğŸš€ Advanced Hosting:\")\n",
    "print(\"   â€¢ Heroku with Procfile\")\n",
    "print(\"   â€¢ AWS EC2 with reverse proxy\")\n",
    "print(\"   â€¢ Kubernetes cluster deployment\")\n",
    "\n",
    "# Performance optimization summary\n",
    "print(\"\\nâš¡ PERFORMANCE OPTIMIZATION SUMMARY:\")\n",
    "print(\"=\" * 45)\n",
    "optimization_tips = [\n",
    "    \"âœ… Use @st.cache_data for data loading and transformations\",\n",
    "    \"âœ… Use @st.cache_resource for ML models and connections\",\n",
    "    \"âœ… Implement session state for user interactions\",\n",
    "    \"âœ… Use st.columns() for efficient layouts\",\n",
    "    \"âœ… Add loading spinners for better UX\",\n",
    "    \"âœ… Monitor performance with timing metrics\",\n",
    "    \"âœ… Clear cache periodically for memory management\",\n",
    "    \"âœ… Use lazy loading for large datasets\",\n",
    "    \"âœ… Optimize Plotly charts with sampling for large data\",\n",
    "    \"âœ… Configure page layout for better performance\"\n",
    "]\n",
    "\n",
    "for tip in optimization_tips:\n",
    "    print(f\"   {tip}\")\n",
    "\n",
    "print(\"\\nğŸ¯ Ready for production-grade dashboard deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cc732e",
   "metadata": {},
   "source": [
    "## ğŸ‰ Module 12 Complete: Interactive Dashboard Mastery\n",
    "\n",
    "### ğŸ“‹ What You've Accomplished\n",
    "\n",
    "âœ… **Streamlit Fundamentals**: Page configuration, layouts, widgets, and styling  \n",
    "âœ… **Interactive Components**: Filters, selectors, date pickers, and real-time updates  \n",
    "âœ… **Advanced Visualizations**: Plotly integration with dynamic chart updates  \n",
    "âœ… **Multi-Page Applications**: Navigation, session state, and user experience  \n",
    "âœ… **Performance Optimization**: Caching strategies, memory management, monitoring  \n",
    "âœ… **Deployment Ready**: Configuration files, requirements, deployment scripts  \n",
    "\n",
    "### ğŸ† Dashboard Portfolio Created\n",
    "\n",
    "**4 Complete Interactive Dashboards:**\n",
    "1. **Sales Analytics Dashboard** - KPIs, filters, time series analysis\n",
    "2. **Scientific Data Explorer** - ML integration, PCA, clustering analysis  \n",
    "3. **Multi-Page Dashboard** - Navigation, tabs, settings management\n",
    "4. **Optimized Performance Dashboard** - Caching, large datasets, monitoring\n",
    "\n",
    "### ğŸ’¡ Key Skills Mastered\n",
    "\n",
    "```python\n",
    "# Essential Streamlit patterns you've learned:\n",
    "STREAMLIT_SKILLS = {\n",
    "    'Caching': '@st.cache_data, @st.cache_resource',\n",
    "    'State Management': 'st.session_state for persistence',\n",
    "    'Layout Control': 'st.columns(), st.container(), st.expander()',\n",
    "    'Interactive Widgets': 'sliders, selectors, date pickers',\n",
    "    'Performance': 'monitoring, optimization, memory management',\n",
    "    'Deployment': 'configuration, requirements, cloud hosting'\n",
    "}\n",
    "```\n",
    "\n",
    "### ğŸš€ Production Deployment Ready\n",
    "\n",
    "Your dashboards are now equipped with:\n",
    "- **Professional configuration** (config.toml)\n",
    "- **Dependency management** (requirements.txt)  \n",
    "- **Deployment automation** (deploy.sh)\n",
    "- **Performance monitoring** and optimization\n",
    "- **Scalable architecture** for large datasets\n",
    "\n",
    "### ğŸ“ˆ Tutorial Progress: 10/14 Complete (71.4%)\n",
    "\n",
    "**ğŸ¯ Next Module**: Data Storytelling and Narrative Design - Learn to craft compelling data narratives that engage and persuade audiences!\n",
    "\n",
    "**Ready to transform your interactive dashboards into powerful storytelling tools!** ğŸ¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56538af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŠ MODULE 12: INTERACTIVE DASHBOARDS & STREAMLIT COMPLETE! ğŸŠ\n",
      "======================================================================\n",
      "\n",
      "ğŸ¯ INTERACTIVE DASHBOARDS CREATED:\n",
      "   âœ… ğŸ“Š Sales Analytics Dashboard - Comprehensive business metrics with filters\n",
      "   âœ… ğŸ”¬ Scientific Data Explorer - ML-powered data analysis and visualization\n",
      "   âœ… ğŸ“š Multi-Page Dashboard - Navigation, tabs, and settings management\n",
      "   âœ… âš¡ Optimized Performance Dashboard - Large dataset handling with caching\n",
      "\n",
      "âš¡ ADVANCED FEATURES IMPLEMENTED:\n",
      "   âœ… Real-time data filtering and interactive controls\n",
      "   âœ… Advanced caching strategies (@st.cache_data, @st.cache_resource)\n",
      "   âœ… Session state management for user persistence\n",
      "   âœ… Performance monitoring and optimization techniques\n",
      "   âœ… Multi-page navigation with seamless UX\n",
      "   âœ… Plotly integration for dynamic visualizations\n",
      "   âœ… Machine learning integration (PCA, clustering)\n",
      "   âœ… Production deployment configuration\n",
      "\n",
      "ğŸ“ FILES GENERATED:\n",
      "   ğŸ“„ scientific_explorer.py         (8.8 KB)\n",
      "   ğŸ“„ sales_dashboard.py             (7.3 KB)\n",
      "   ğŸ“„ requirements.txt               (0.1 KB)\n",
      "   ğŸ“„ optimized_dashboard.py         (8.7 KB)\n",
      "   ğŸ“„ deploy.sh                      (0.8 KB)\n",
      "   ğŸ“„ multipage_dashboard.py         (7.0 KB)\n",
      "\n",
      "   ğŸ“ Configuration: .streamlit/config.toml\n",
      "\n",
      "ğŸš€ DEPLOYMENT OPTIONS:\n",
      "   ğŸ–¥ï¸  Local: streamlit run dashboard_name.py\n",
      "   â˜ï¸  Streamlit Cloud: share.streamlit.io (GitHub integration)\n",
      "   ğŸ³ Docker: Containerized deployment for cloud platforms\n",
      "   ğŸŒ Production: Heroku, AWS, GCP, Azure hosting\n",
      "\n",
      "ğŸ“ˆ TUTORIAL PROGRESS:\n",
      "   ğŸ“š Module 12/14 Complete (85.7% done)\n",
      "   ğŸ¯ Next: Module 13 - Data Storytelling and Narrative Design\n",
      "\n",
      "ğŸª You've mastered interactive dashboard development!\n",
      "   Next: Learn to craft compelling data stories that persuade and engage! ğŸ¬\n",
      "\n",
      "ğŸ’¾ TOTAL PROJECT SIZE:\n",
      "   ğŸ“Š Generated content: 170.5 MB\n",
      "   ğŸ“„ Interactive dashboards: 4 complete applications\n",
      "   âš¡ Performance optimized for production deployment\n"
     ]
    }
   ],
   "source": [
    "# Module 12 Completion Summary\n",
    "print(\"ğŸŠ MODULE 12: INTERACTIVE DASHBOARDS & STREAMLIT COMPLETE! ğŸŠ\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Dashboards created\n",
    "dashboards_created = [\n",
    "    \"ğŸ“Š Sales Analytics Dashboard - Comprehensive business metrics with filters\",\n",
    "    \"ğŸ”¬ Scientific Data Explorer - ML-powered data analysis and visualization\",\n",
    "    \"ğŸ“š Multi-Page Dashboard - Navigation, tabs, and settings management\",\n",
    "    \"âš¡ Optimized Performance Dashboard - Large dataset handling with caching\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ¯ INTERACTIVE DASHBOARDS CREATED:\")\n",
    "for dashboard in dashboards_created:\n",
    "    print(f\"   âœ… {dashboard}\")\n",
    "\n",
    "print()\n",
    "print(\"âš¡ ADVANCED FEATURES IMPLEMENTED:\")\n",
    "advanced_features = [\n",
    "    \"âœ… Real-time data filtering and interactive controls\",\n",
    "    \"âœ… Advanced caching strategies (@st.cache_data, @st.cache_resource)\",\n",
    "    \"âœ… Session state management for user persistence\", \n",
    "    \"âœ… Performance monitoring and optimization techniques\",\n",
    "    \"âœ… Multi-page navigation with seamless UX\",\n",
    "    \"âœ… Plotly integration for dynamic visualizations\",\n",
    "    \"âœ… Machine learning integration (PCA, clustering)\",\n",
    "    \"âœ… Production deployment configuration\"\n",
    "]\n",
    "\n",
    "for feature in advanced_features:\n",
    "    print(f\"   {feature}\")\n",
    "\n",
    "print()\n",
    "print(\"ğŸ“ FILES GENERATED:\")\n",
    "import os\n",
    "dashboard_files = []\n",
    "dashboard_dir = '/Users/sanjeevadodlapati/Downloads/Repos/DataVisualization-Comprehensive-Tutorial/outputs/dashboards'\n",
    "for file in os.listdir(dashboard_dir):\n",
    "    if file.endswith('.py') or file.endswith('.txt') or file.endswith('.sh'):\n",
    "        file_size = os.path.getsize(os.path.join(dashboard_dir, file)) / 1024\n",
    "        dashboard_files.append(f\"ğŸ“„ {file:<30} ({file_size:.1f} KB)\")\n",
    "\n",
    "for file_info in dashboard_files:\n",
    "    print(f\"   {file_info}\")\n",
    "\n",
    "print(f\"\\n   ğŸ“ Configuration: .streamlit/config.toml\")\n",
    "\n",
    "print()\n",
    "print(\"ğŸš€ DEPLOYMENT OPTIONS:\")\n",
    "deployment_options = [\n",
    "    \"ğŸ–¥ï¸  Local: streamlit run dashboard_name.py\",\n",
    "    \"â˜ï¸  Streamlit Cloud: share.streamlit.io (GitHub integration)\",\n",
    "    \"ğŸ³ Docker: Containerized deployment for cloud platforms\",\n",
    "    \"ğŸŒ Production: Heroku, AWS, GCP, Azure hosting\"\n",
    "]\n",
    "\n",
    "for option in deployment_options:\n",
    "    print(f\"   {option}\")\n",
    "\n",
    "print()\n",
    "print(\"ğŸ“ˆ TUTORIAL PROGRESS:\")\n",
    "print(\"   ğŸ“š Module 12/14 Complete (85.7% done)\")\n",
    "print(\"   ğŸ¯ Next: Module 13 - Data Storytelling and Narrative Design\")\n",
    "print()\n",
    "print(\"ğŸª You've mastered interactive dashboard development!\")\n",
    "print(\"   Next: Learn to craft compelling data stories that persuade and engage! ğŸ¬\")\n",
    "\n",
    "# Performance summary\n",
    "print(f\"\\nğŸ’¾ TOTAL PROJECT SIZE:\")\n",
    "total_size = 0\n",
    "for root, dirs, files in os.walk('/Users/sanjeevadodlapati/Downloads/Repos/DataVisualization-Comprehensive-Tutorial/outputs'):\n",
    "    for file in files:\n",
    "        total_size += os.path.getsize(os.path.join(root, file))\n",
    "\n",
    "print(f\"   ğŸ“Š Generated content: {total_size / (1024*1024):.1f} MB\")\n",
    "print(f\"   ğŸ“„ Interactive dashboards: 4 complete applications\")\n",
    "print(f\"   âš¡ Performance optimized for production deployment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9069b0d",
   "metadata": {},
   "source": [
    "## ğŸ“± In-Notebook Interactive Dashboards\n",
    "\n",
    "While Streamlit dashboards are great for web deployment, we can also create interactive dashboards that render directly in Jupyter notebooks using **ipywidgets** and **Plotly**. This gives you the best of both worlds!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "986afc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“± Creating In-Notebook Interactive Dashboard...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "073b970d56f341e1ba5fc6e4d1e9e808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>ğŸ“Š Interactive Sales Dashboard</h2><p>Use the controls below to filter and exploâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Interactive Dashboard Created!\n",
      "ğŸ® Use the controls above to filter and explore the data in real-time!\n"
     ]
    }
   ],
   "source": [
    "# Interactive Dashboard Components with ipywidgets\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "print(\"ğŸ“± Creating In-Notebook Interactive Dashboard...\")\n",
    "\n",
    "# Create interactive widgets\n",
    "date_picker = widgets.DatePicker(\n",
    "    value=sales_data['date'].max().date(),\n",
    "    description='End Date:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "region_selector = widgets.SelectMultiple(\n",
    "    options=list(sales_data['region'].unique()),\n",
    "    value=list(sales_data['region'].unique()),\n",
    "    description='Regions:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(height='100px')\n",
    ")\n",
    "\n",
    "product_selector = widgets.SelectMultiple(\n",
    "    options=list(sales_data['product'].unique()),\n",
    "    value=list(sales_data['product'].unique()),\n",
    "    description='Products:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(height='100px')\n",
    ")\n",
    "\n",
    "days_back_slider = widgets.IntSlider(\n",
    "    value=90,\n",
    "    min=30,\n",
    "    max=365,\n",
    "    step=30,\n",
    "    description='Days Back:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "chart_type_dropdown = widgets.Dropdown(\n",
    "    options=['Line Chart', 'Bar Chart', 'Area Chart'],\n",
    "    value='Line Chart',\n",
    "    description='Chart Type:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Create output widget for the dashboard\n",
    "output_widget = widgets.Output()\n",
    "\n",
    "def update_dashboard(*args):\n",
    "    \"\"\"Function to update dashboard when widgets change\"\"\"\n",
    "    with output_widget:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        # Filter data based on widget selections\n",
    "        end_date = pd.to_datetime(date_picker.value)\n",
    "        start_date = end_date - pd.Timedelta(days=days_back_slider.value)\n",
    "        \n",
    "        filtered_data = sales_data[\n",
    "            (sales_data['date'] >= start_date) &\n",
    "            (sales_data['date'] <= end_date) &\n",
    "            (sales_data['region'].isin(region_selector.value)) &\n",
    "            (sales_data['product'].isin(product_selector.value))\n",
    "        ]\n",
    "        \n",
    "        if len(filtered_data) == 0:\n",
    "            print(\"âš ï¸ No data matches the selected filters\")\n",
    "            return\n",
    "        \n",
    "        # Create dashboard layout\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=[\n",
    "                'Sales Trend Over Time',\n",
    "                'Sales by Region', \n",
    "                'Product Performance',\n",
    "                'Daily Sales Distribution'\n",
    "            ],\n",
    "            specs=[[{\"type\": \"xy\"}, {\"type\": \"domain\"}],\n",
    "                   [{\"type\": \"xy\"}, {\"type\": \"xy\"}]]\n",
    "        )\n",
    "        \n",
    "        # 1. Time series chart (top left)\n",
    "        daily_sales = filtered_data.groupby('date')['sales'].sum().reset_index()\n",
    "        \n",
    "        if chart_type_dropdown.value == 'Line Chart':\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=daily_sales['date'], y=daily_sales['sales'],\n",
    "                          mode='lines+markers', name='Sales', line=dict(width=3)),\n",
    "                row=1, col=1\n",
    "            )\n",
    "        elif chart_type_dropdown.value == 'Bar Chart':\n",
    "            fig.add_trace(\n",
    "                go.Bar(x=daily_sales['date'], y=daily_sales['sales'], name='Sales'),\n",
    "                row=1, col=1\n",
    "            )\n",
    "        else:  # Area Chart\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=daily_sales['date'], y=daily_sales['sales'],\n",
    "                          fill='tozeroy', name='Sales'),\n",
    "                row=1, col=1\n",
    "            )\n",
    "        \n",
    "        # 2. Regional pie chart (top right)\n",
    "        regional_sales = filtered_data.groupby('region')['sales'].sum()\n",
    "        fig.add_trace(\n",
    "            go.Pie(labels=regional_sales.index, values=regional_sales.values, \n",
    "                   name=\"Regional Sales\"),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # 3. Product bar chart (bottom left)\n",
    "        product_sales = filtered_data.groupby('product')['sales'].sum().sort_values(ascending=True)\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=product_sales.values, y=product_sales.index, \n",
    "                   orientation='h', name='Product Sales'),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # 4. Sales distribution histogram (bottom right)\n",
    "        fig.add_trace(\n",
    "            go.Histogram(x=filtered_data['sales'], nbinsx=20, \n",
    "                        name='Sales Distribution'),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            height=800,\n",
    "            title_text=f\"Interactive Sales Dashboard ({len(filtered_data):,} records)\",\n",
    "            title_x=0.5,\n",
    "            showlegend=False\n",
    "        )\n",
    "        \n",
    "        # Display metrics\n",
    "        total_sales = filtered_data['sales'].sum()\n",
    "        avg_daily_sales = daily_sales['sales'].mean()\n",
    "        max_daily_sales = daily_sales['sales'].max()\n",
    "        \n",
    "        print(\"ğŸ“Š KEY METRICS\")\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"ğŸ’° Total Sales: ${total_sales:,.0f}\")\n",
    "        print(f\"ğŸ“ˆ Average Daily: ${avg_daily_sales:,.0f}\")\n",
    "        print(f\"ğŸ”¥ Peak Daily: ${max_daily_sales:,.0f}\")\n",
    "        print(f\"ğŸ“… Date Range: {start_date.date()} to {end_date.date()}\")\n",
    "        print(f\"ğŸ“Š Records: {len(filtered_data):,}\")\n",
    "        print()\n",
    "        \n",
    "        # Show the interactive plot\n",
    "        fig.show()\n",
    "\n",
    "# Connect widgets to update function\n",
    "date_picker.observe(update_dashboard, names='value')\n",
    "region_selector.observe(update_dashboard, names='value')\n",
    "product_selector.observe(update_dashboard, names='value')\n",
    "days_back_slider.observe(update_dashboard, names='value')\n",
    "chart_type_dropdown.observe(update_dashboard, names='value')\n",
    "\n",
    "# Create dashboard layout\n",
    "dashboard_title = widgets.HTML(\n",
    "    value=\"<h2>ğŸ“Š Interactive Sales Dashboard</h2><p>Use the controls below to filter and explore the data:</p>\",\n",
    "    layout=widgets.Layout(margin='0 0 20px 0')\n",
    ")\n",
    "\n",
    "controls_box = widgets.VBox([\n",
    "    dashboard_title,\n",
    "    widgets.HBox([\n",
    "        widgets.VBox([date_picker, days_back_slider], layout=widgets.Layout(margin='0 20px 0 0')),\n",
    "        widgets.VBox([region_selector], layout=widgets.Layout(margin='0 20px 0 0')),\n",
    "        widgets.VBox([product_selector], layout=widgets.Layout(margin='0 20px 0 0')),\n",
    "        widgets.VBox([chart_type_dropdown])\n",
    "    ]),\n",
    "    output_widget\n",
    "], layout=widgets.Layout(border='2px solid #ddd', padding='20px', margin='10px'))\n",
    "\n",
    "# Display the dashboard\n",
    "display(controls_box)\n",
    "\n",
    "# Initial load\n",
    "update_dashboard()\n",
    "\n",
    "print(\"âœ… Interactive Dashboard Created!\")\n",
    "print(\"ğŸ® Use the controls above to filter and explore the data in real-time!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345d9067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¬ Creating Advanced Scientific Dashboard...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5737341be5ca47c7813749ab8fe2ece8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>ğŸ”¬ Scientific Data Analysis Dashboard</h2><p>Explore datasets with PCA and clustâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scientific Dashboard Created!\n",
      "ğŸ§ª Explore PCA, clustering, and correlation analysis interactively!\n"
     ]
    }
   ],
   "source": [
    "# Advanced Scientific Dashboard with ML Integration\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"\\nğŸ”¬ Creating Advanced Scientific Dashboard...\")\n",
    "\n",
    "# Scientific dashboard widgets\n",
    "dataset_dropdown = widgets.Dropdown(\n",
    "    options=['Iris Dataset', 'Customer Analytics'],\n",
    "    value='Iris Dataset',\n",
    "    description='Dataset:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "analysis_tabs = widgets.Tab()\n",
    "analysis_tabs.children = [\n",
    "    widgets.VBox([]),  # PCA tab\n",
    "    widgets.VBox([]),  # Clustering tab  \n",
    "    widgets.VBox([])   # Statistics tab\n",
    "]\n",
    "analysis_tabs.titles = ['PCA Analysis', 'Clustering', 'Statistics']\n",
    "\n",
    "# PCA controls\n",
    "pca_components_slider = widgets.IntSlider(\n",
    "    value=2,\n",
    "    min=2,\n",
    "    max=4,\n",
    "    description='Components:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Clustering controls\n",
    "n_clusters_slider = widgets.IntSlider(\n",
    "    value=3,\n",
    "    min=2,\n",
    "    max=8,\n",
    "    description='Clusters:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "feature_selector = widgets.SelectMultiple(\n",
    "    options=[],\n",
    "    value=[],\n",
    "    description='Features:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(height='120px')\n",
    ")\n",
    "\n",
    "# Scientific output widget\n",
    "scientific_output = widgets.Output()\n",
    "\n",
    "def update_scientific_dashboard(*args):\n",
    "    \"\"\"Update scientific dashboard based on widget selections\"\"\"\n",
    "    with scientific_output:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        # Select dataset\n",
    "        if dataset_dropdown.value == 'Iris Dataset':\n",
    "            data = iris_df.copy()\n",
    "            numeric_cols = ['sepal length (cm)', 'sepal width (cm)', \n",
    "                          'petal length (cm)', 'petal width (cm)']\n",
    "            target_col = 'species'\n",
    "        else:  # Customer Analytics\n",
    "            data = customer_data.copy()\n",
    "            numeric_cols = ['age', 'income', 'spending', 'satisfaction']\n",
    "            target_col = 'segment'\n",
    "        \n",
    "        # Update feature selector options\n",
    "        if list(feature_selector.options) != numeric_cols:\n",
    "            feature_selector.options = numeric_cols\n",
    "            feature_selector.value = numeric_cols[:4] if len(numeric_cols) >= 4 else numeric_cols\n",
    "        \n",
    "        selected_features = list(feature_selector.value)\n",
    "        if len(selected_features) < 2:\n",
    "            print(\"âš ï¸ Please select at least 2 features for analysis\")\n",
    "            return\n",
    "        \n",
    "        # Prepare data\n",
    "        X = data[selected_features]\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # Create subplots\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=[\n",
    "                'PCA Visualization',\n",
    "                'K-Means Clustering',\n",
    "                'Feature Correlation',\n",
    "                'Distribution Comparison'\n",
    "            ],\n",
    "            specs=[[{\"type\": \"xy\"}, {\"type\": \"xy\"}],\n",
    "                   [{\"type\": \"xy\"}, {\"type\": \"xy\"}]]\n",
    "        )\n",
    "        \n",
    "        # 1. PCA Analysis\n",
    "        n_components = min(pca_components_slider.value, len(selected_features))\n",
    "        pca = PCA(n_components=n_components)\n",
    "        X_pca = pca.fit_transform(X_scaled)\n",
    "        \n",
    "        pca_df = pd.DataFrame(X_pca[:, :2], columns=['PC1', 'PC2'])\n",
    "        pca_df[target_col] = data[target_col].values\n",
    "        \n",
    "        for category in pca_df[target_col].unique():\n",
    "            mask = pca_df[target_col] == category\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=pca_df.loc[mask, 'PC1'],\n",
    "                    y=pca_df.loc[mask, 'PC2'],\n",
    "                    mode='markers',\n",
    "                    name=f'PCA - {category}',\n",
    "                    legendgroup='pca'\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "        \n",
    "        # 2. K-Means Clustering\n",
    "        kmeans = KMeans(n_clusters=n_clusters_slider.value, random_state=42)\n",
    "        clusters = kmeans.fit_predict(X_scaled)\n",
    "        \n",
    "        cluster_df = pd.DataFrame(X_pca[:, :2], columns=['PC1', 'PC2'])\n",
    "        cluster_df['cluster'] = [f'Cluster {i}' for i in clusters]\n",
    "        \n",
    "        for cluster in cluster_df['cluster'].unique():\n",
    "            mask = cluster_df['cluster'] == cluster\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=cluster_df.loc[mask, 'PC1'],\n",
    "                    y=cluster_df.loc[mask, 'PC2'],\n",
    "                    mode='markers',\n",
    "                    name=cluster,\n",
    "                    legendgroup='clustering'\n",
    "                ),\n",
    "                row=1, col=2\n",
    "            )\n",
    "        \n",
    "        # Add cluster centers\n",
    "        centers_pca = pca.transform(scaler.inverse_transform(kmeans.cluster_centers_))\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=centers_pca[:, 0],\n",
    "                y=centers_pca[:, 1],\n",
    "                mode='markers',\n",
    "                marker=dict(symbol='x', size=15, color='black'),\n",
    "                name='Centers',\n",
    "                legendgroup='clustering'\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # 3. Feature Correlation Heatmap\n",
    "        if len(selected_features) >= 2:\n",
    "            corr_matrix = X.corr()\n",
    "            fig.add_trace(\n",
    "                go.Heatmap(\n",
    "                    z=corr_matrix.values,\n",
    "                    x=corr_matrix.columns,\n",
    "                    y=corr_matrix.columns,\n",
    "                    colorscale='RdBu',\n",
    "                    zmid=0,\n",
    "                    showscale=True\n",
    "                ),\n",
    "                row=2, col=1\n",
    "            )\n",
    "        \n",
    "        # 4. Distribution comparison (box plots)\n",
    "        if len(selected_features) >= 1:\n",
    "            feature_to_plot = selected_features[0]\n",
    "            for category in data[target_col].unique():\n",
    "                category_data = data[data[target_col] == category][feature_to_plot]\n",
    "                fig.add_trace(\n",
    "                    go.Box(\n",
    "                        y=category_data,\n",
    "                        name=category,\n",
    "                        legendgroup='distributions'\n",
    "                    ),\n",
    "                    row=2, col=2\n",
    "                )\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            height=800,\n",
    "            title_text=f\"Scientific Data Analysis: {dataset_dropdown.value}\",\n",
    "            title_x=0.5\n",
    "        )\n",
    "        \n",
    "        # Display analysis metrics\n",
    "        print(\"ğŸ”¬ ANALYSIS RESULTS\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"ğŸ“Š Dataset: {dataset_dropdown.value}\")\n",
    "        print(f\"ğŸ” Features Analyzed: {len(selected_features)}\")\n",
    "        print(f\"ğŸ“ˆ PCA Components: {n_components}\")\n",
    "        print(f\"ğŸ¯ Clusters: {n_clusters_slider.value}\")\n",
    "        print()\n",
    "        \n",
    "        print(\"ğŸ“Š PCA Explained Variance:\")\n",
    "        for i, var_ratio in enumerate(pca.explained_variance_ratio_):\n",
    "            print(f\"   PC{i+1}: {var_ratio:.3f} ({var_ratio*100:.1f}%)\")\n",
    "        \n",
    "        print(f\"   Cumulative: {pca.explained_variance_ratio_.sum():.3f} ({pca.explained_variance_ratio_.sum()*100:.1f}%)\")\n",
    "        print()\n",
    "        \n",
    "        print(\"ğŸ¯ Clustering Quality:\")\n",
    "        silhouette_avg = \"N/A\"  # Would need silhouette_score for full implementation\n",
    "        print(f\"   Inertia: {kmeans.inertia_:.2f}\")\n",
    "        print(f\"   Iterations: {kmeans.n_iter_}\")\n",
    "        print()\n",
    "        \n",
    "        # Show the plot\n",
    "        fig.show()\n",
    "\n",
    "# Connect scientific widgets\n",
    "dataset_dropdown.observe(update_scientific_dashboard, names='value')\n",
    "pca_components_slider.observe(update_scientific_dashboard, names='value')\n",
    "n_clusters_slider.observe(update_scientific_dashboard, names='value')\n",
    "feature_selector.observe(update_scientific_dashboard, names='value')\n",
    "\n",
    "# Scientific dashboard layout\n",
    "scientific_title = widgets.HTML(\n",
    "    value=\"<h2>ğŸ”¬ Scientific Data Analysis Dashboard</h2><p>Explore datasets with PCA and clustering analysis:</p>\",\n",
    "    layout=widgets.Layout(margin='0 0 20px 0')\n",
    ")\n",
    "\n",
    "scientific_controls = widgets.VBox([\n",
    "    scientific_title,\n",
    "    widgets.HBox([\n",
    "        widgets.VBox([dataset_dropdown, pca_components_slider], \n",
    "                    layout=widgets.Layout(margin='0 20px 0 0')),\n",
    "        widgets.VBox([n_clusters_slider], \n",
    "                    layout=widgets.Layout(margin='0 20px 0 0')),\n",
    "        widgets.VBox([feature_selector])\n",
    "    ]),\n",
    "    scientific_output\n",
    "], layout=widgets.Layout(border='2px solid #ddd', padding='20px', margin='20px 0'))\n",
    "\n",
    "# Display scientific dashboard\n",
    "display(scientific_controls)\n",
    "\n",
    "# Initial load for scientific dashboard\n",
    "update_scientific_dashboard()\n",
    "\n",
    "print(\"âœ… Scientific Dashboard Created!\")\n",
    "print(\"ğŸ§ª Explore PCA, clustering, and correlation analysis interactively!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49abbcde",
   "metadata": {},
   "source": [
    "## ğŸ¯ Dashboard Approaches Comparison\n",
    "\n",
    "We've covered both **file-based dashboards** (Streamlit) and **notebook-based dashboards** (ipywidgets):\n",
    "\n",
    "### ğŸŒ Streamlit Dashboards (File-Based)\n",
    "**Best for:** Production deployment, sharing with stakeholders, web applications\n",
    "- âœ… Professional web interface\n",
    "- âœ… Easy deployment to cloud platforms\n",
    "- âœ… Multi-page applications\n",
    "- âœ… Built-in caching and performance optimization\n",
    "- âœ… User authentication and session management\n",
    "\n",
    "### ğŸ““ IPywidgets Dashboards (Notebook-Based)\n",
    "**Best for:** Exploratory analysis, research, interactive presentations\n",
    "- âœ… Immediate rendering in notebooks\n",
    "- âœ… No separate files needed\n",
    "- âœ… Perfect for Jupyter environments\n",
    "- âœ… Great for prototyping and exploration\n",
    "- âœ… Integrates seamlessly with analysis workflow\n",
    "\n",
    "### ğŸ® Interactive Features Available:\n",
    "- **Real-time filtering** (dates, categories, numerical ranges)\n",
    "- **Dynamic chart types** (scatter, line, bar, heatmap)\n",
    "- **Machine learning integration** (PCA, clustering)\n",
    "- **Statistical analysis** (correlations, distributions)\n",
    "- **Multi-dataset support** (easy switching between datasets)\n",
    "\n",
    "## ğŸš€ Next Steps & Advanced Topics\n",
    "\n",
    "### ğŸ“ˆ Performance Optimization\n",
    "- Implement data sampling for large datasets\n",
    "- Use progressive loading for complex visualizations\n",
    "- Optimize memory usage with chunked processing\n",
    "\n",
    "### ğŸ¨ Advanced Styling\n",
    "- Custom CSS for Streamlit applications\n",
    "- Professional dashboard themes\n",
    "- Responsive design for mobile devices\n",
    "\n",
    "### ğŸ”— Integration Opportunities\n",
    "- Database connections (SQL, MongoDB)\n",
    "- API integrations for live data\n",
    "- Cloud storage integration (AWS S3, Google Cloud)\n",
    "- Version control for dashboard configurations\n",
    "\n",
    "### ğŸ“Š Enterprise Features\n",
    "- User authentication and role-based access\n",
    "- Audit logging and usage analytics\n",
    "- Automated report generation\n",
    "- Email notifications and alerts\n",
    "\n",
    "## ğŸ“ Module 12 Complete!\n",
    "\n",
    "You now have the skills to create:\n",
    "- âœ… Professional Streamlit web dashboards\n",
    "- âœ… Interactive notebook-based dashboards\n",
    "- âœ… Scientific analysis tools with ML integration\n",
    "- âœ… Performance-optimized applications\n",
    "- âœ… Deployment-ready configurations\n",
    "\n",
    "**Ready for Module 13: Data Storytelling & Narrative Design! ğŸ“–**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataviz_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
